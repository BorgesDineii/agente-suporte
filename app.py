import streamlit as st
import requests
from google import genai # Mudan√ßa para o import correto
import os
from bs4 import BeautifulSoup
import base64
import json
import numpy as np # Adicionado
import faiss # Adicionado
from langchain_text_splitters import RecursiveCharacterTextSplitter # Adicionado

# --- 1. CONFIGURA√á√ïES E VARI√ÅVEIS ---
# ATEN√á√ÉO: √â recomendado usar st.secrets ou vari√°veis de ambiente para estas chaves.
# Deixei como vari√°veis diretas para fins de restaura√ß√£o, mas remova antes de fazer commit!
api_key = "xxxx"
ATLASSIAN_USER = "valdinei.borges@e-deploy.com.br"
ATLASSIAN_TOKEN = "xxxx-HeQXotkpCj3tN1LzABhvv0MaI2GkZqDoTII98=FA994E2B"
CONFLUENCE_URL = "https://edeploy.atlassian.net"

CONFLUENCE_URL = "https://edeploy.atlassian.net"
USER_EMAIL = "valdinei.borges@e-deploy.com.br"
API_TOKEN = "xxx-HeQXotkpCj3tN1LzABhvv0MaI2GkZqDoTII98=FA994E2B"
SPACE_KEY = "SPOS2"

if not all([CONFLUENCE_URL, USER_EMAIL, API_TOKEN]):
    st.error("ERRO: Configure as variaveis de ambiente (ATLASSIAN_USER, ATLASSIAN_TOKEN e CONFLUENCE_URL).")
    st.stop()

# Inicializa√ß√£o do cliente Gemini
try:
    client = genai.Client(api_key=api_key)
except Exception as e:
    st.error(f"Erro ao iniciar o cliente Google GenAI: {e}")
    st.stop()


# --- 2. FUN√á√ïES DE INGEST√ÉO E LIMPEZA DE DADOS ---

def get_auth_headers():
    """
    Cria um cabe√ßalho de autentica√ß√£o (Basic Auth).
    """
    auth_string = f"{USER_EMAIL}:{API_TOKEN}"
    encoded_auth = base64.b64encode(auth_string.encode()).decode()
    return{
        "Authorization": f"Basic {encoded_auth}",
        "Accept":"application/json"
    }

def limpando_html_content(html_content):
    """
    Remove tags HTML e limpa o texto do conteudo do Confluence.
    """
    soup = BeautifulSoup(html_content, 'html.parser')

    for tag in soup(["nav", "header", "footer", "style", "script"]):
        tag.decompose()

    return soup.get_text(separator=' ', strip=True)

def busca_conteudo_confluence(space_key):
    """
    Busca todas as p√°ginas de um Space Key e extrai o conte√∫do limpo.
    """
    # Usando o m√©todo mais robusto (auth=(email, token)) para evitar problemas de Base64
    auth_credentials = (USER_EMAIL.strip(), API_TOKEN.strip()) 
    headers = {"Accept":"application/json"}
    
    url = f"{CONFLUENCE_URL}/wiki/rest/api/content?spaceKey={space_key}&expand=body.storage&limit=25"
    clean_knowledge_base = []

    try:
        # CORRE√á√ÉO: Usando 'auth' no requests para autentica√ß√£o direta
        response = requests.get(url, headers=headers, auth=auth_credentials) 
        response.raise_for_status()

        data = response.json()
        st.success(f"‚úÖ Conectado ao Confluence. Encontradas {len(data.get('results',[]))} p√°ginas.")
        
        for page in data.get('results', []):
            title = page.get('title')
            html_content = page.get('body', {}).get('storage', {}).get('value', '')

            if html_content:
                # CORRE√á√ÉO: Alterando a chamada da fun√ß√£o para 'limpando_html_content'
                clean_text = limpando_html_content(html_content) 
                clean_knowledge_base.append({
                    "title":title,
                    "text":clean_text
                })
        return clean_knowledge_base

    except requests.exceptions.HTTPError as e:
        st.error(f"‚ùå Erro HTTP ao conectar: {e}. Verifique seu Token.")
        return None
    except Exception as e:
        st.error(f"‚ùå Erro desconhecido: {e}")
        return None
        
def create_vector_store(knowledge_base, client):
    """
    Divide o texto em chunks, gera embeddings e cria o √≠ndice vetorial FAISS.
    """
    documents = []
    
    # 1. Chunking (Divis√£o do texto)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=300, 
        length_function=len 
    )

    for item in knowledge_base:
        chunks = text_splitter.split_text(item['text'])
        for chunk in chunks:
            documents.append({
                "title": item['title'],
                "chunk": chunk,
                "text": chunk 
            })
    
    # 2. Embedding (Gera√ß√£o de Vetores)
    texts = [doc["text"] for doc in documents]
    
    try:
        response = client.models.embed_content(
            model='text-embedding-004', 
            contents=texts 
        )

        raw_embeddings_list = [item.values for item in response.embeddings]

        # CORRE√á√ÉO APLICADA AQUI: Nota√ß√£o de Ponto
        embeddings = np.array(raw_embeddings_list, dtype=np.float32)

        if embeddings.ndim == 1:
            embeddings = embeddings.reshape(1, -1)

    except Exception as e:
        st.error(f"‚ùå Erro no Embedding do Gemini: {e}")
        return None, None

    # 3. FAISS (Cria√ß√£o do √çndice Vetorial)
    d = embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(embeddings)

    return index, documents


# --- 3. FUN√á√ÉO DE RESPOSTA RAG ---

def gerar_resposta_rag(user_query, vector_index, documents, client):
    """
    Busca o contexto relevante no √≠ndice FAISS e usa o Gemini para gerar uma resposta.
    """
    # 1. Recupera√ß√£o (Retrieval)

    context = ""

    # üõë ADICIONE ESTE BLOCO DE DEBUG üõë
    import streamlit as st
    st.sidebar.markdown("---")
    st.sidebar.subheader("Contexto Recuperado (DEBUG):")
    st.sidebar.markdown(context) 
    # üõë FIM DO BLOCO DE DEBUG üõë
    
    # Cria o embedding da pergunta do usu√°rio
    query_embedding_response = client.models.embed_content(
        model='text-embedding-004',
        contents=[user_query]
    )

    raw_query_vector = query_embedding_response.embeddings[0].values

    # CORRE√á√ÉO APLICADA AQUI: Nota√ß√£o de Ponto
    query_embedding = np.array(raw_query_vector, dtype=np.float32)

    # Busca os 3 chunks mais relevantes
    D, I = vector_index.search(query_embedding.reshape(1, -1), k=10) 

    valid_indices = [i for i in I[0] if i != -1]

    if not valid_indices:
        return "Desculpe, nao encontrei informa√ß√µes relevantes na Base de conhecimento para esta busca."
    
    retrieved_texts = [documents[i]['text'] for i in valid_indices]

    context = "\n---\n".join(retrieved_texts)

    # Constr√≥i o contexto com o texto dos chunks recuperados
    # retrieved_texts = [documents[i]['text'] for i in I[0] if i != -1]
    
    #if not retrieved_texts:
    #    return "Desculpe meu nobre, n√£o encontrei informa√ß√µes relevantes na Base de Conhecimento de Suporte"

    # context = "\n---\n".join(retrieved_texts)

    # 2. Prompt Engineering
    system_instruction = (
    """
    Voc√™ √© o Rodrigo GPT, um Agente de Suporte T√©cnico da E-DEPLOY. Sua fun√ß√£o √© ser proativo, respeitoso e fornecer solu√ß√µes e procedimentos claros.

    REGRAS OBRIGAT√ìRIAS DE RESPOSTA (RAG):
    1. Utilize **APENAS** as informa√ß√µes contidas no 'CONTEXTO DE PROCEDIMENTO' fornecido para gerar sua resposta.
    2. Se o CONTEXTO contiver o passo a passo de um procedimento, estruture sua resposta em **passos claros e numerados (Ex: 1. Acessar..., 2. Clicar..., etc.)**.
    3. Mantenha a resposta objetiva, focando na solu√ß√£o.

    LIMITE DE CONHECIMENTO:
    1. **Sua √∫nica exce√ß√£o para n√£o responder √© a aus√™ncia total de informa√ß√£o.** Se o CONTEXTO estiver vazio, voc√™ deve responder: "N√£o encontrei informa√ß√µes relevantes para esta busca."
    2. Caso receba contexto, **voc√™ deve gerar a resposta com base nele, mesmo que a informa√ß√£o seja parcial ou incompleta**.
    3. N√£o responda a perguntas sobre sa√∫de, medicamentos ou quest√µes jur√≠dicas.

    PERSONA:
    - Se a pergunta for "quem √© voce?", responda: "Ol√°, sou Rodrigo GPT, um grande f√£ de churros e comida."
    """)

    prompt = (
        f"INSTRU√á√ÉO: {system_instruction}\n\n"
        f"CONTEXTO DE PROCEDIMENTO:\n{context}\n\n"
        f"PERGUNTA DO USU√ÅRIO: {user_query}"
    )

    # 3. Gera√ß√£o
    response = client.models.generate_content(
        model='gemini-2.5-flash',
        contents=prompt
    )
    return response.text

# --- 4. L√ìGICA PRINCIPAL DO STREAMLIT ---

st.set_page_config(
    page_title="Agente de Suporte para procedimento e corre√ß√£o de problemas relacionados ao sistema",
    layout="wide"
)

st.title("Agente de Suporte (Rodrigo GPTü§ìüêã)")
st.markdown("Ol√°!! Sou Rodrigo GPT, seu assistente de suporte para consulta de d√∫vidas e procedimento.")
st.markdown("---")

# Removendo st.text_area isolado, pois a entrada de chat √© mais eficiente

# Fun√ß√£o get_respondendo_pergunta removida pois n√£o era usada

if 'vector_index' not in st.session_state:
    # 1. INGEST√ÉO/CARREGAMENTO
    with st.spinner("‚è≥ Carregando ou Ingerindo Base de Conhecimento do Confluence..."):
        
        knowledge_base = busca_conteudo_confluence(SPACE_KEY)
        
        if knowledge_base:
            try:
                vector_index, documents = create_vector_store(knowledge_base, client)
                
                st.session_state['vector_index'] = vector_index
                st.session_state['documents'] = documents
                st.success("‚úÖ Base de Conhecimento Carregada com Sucesso!")
            
            except Exception as e:
                st.error(f"‚ùå Erro fatal durante o Embedding ou FAISS: {e}")
                st.stop()
        else:
            st.error("‚ùå Erro fatal: N√£o foi poss√≠vel carregar o conte√∫do do Confluence.")
            st.stop()

# Garante que as vari√°veis estejam dispon√≠veis
vector_index = st.session_state['vector_index']
documents = st.session_state['documents']

# 2. L√ìGICA DO CHAT

if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe o hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a entrada do usu√°rio
if prompt := st.chat_input("Pergunte sobre um procedimento ou erro..."):
    
    # CORRE√á√ÉO: st.session_state.messages.append
    st.session_state.messages.append({"role": "user", "content": prompt}) 
    
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner("ü§ñ Buscando e analisando procedimentos..."):
        try:
            full_response = gerar_resposta_rag(prompt, vector_index, documents, client)
        except Exception as e:
            full_response = f"Ocorreu um erro ao gerar a resposta: {e}"

    with st.chat_message("assistant"):
        st.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})


# O bloco 'if __name__ == "__main__":' foi removido e sua l√≥gica integrada ao Streamlit.
# A fun√ß√£o de chat do Streamlit j√° √© o bloco principal de execu√ß√£o.